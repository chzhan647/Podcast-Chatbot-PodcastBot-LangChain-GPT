{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import os \n",
    "# from openai.embeddings_utils import get_embedding\n",
    "from openai import OpenAI\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "import podcast_scraper\n",
    "from pinecone import Pinecone\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.schema import Document\n",
    "\n",
    "from langchain_pinecone import PineconeVectorStore\n",
    "\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "\n",
    "from langchain.chat_models import ChatOpenAI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Building the recommendation system using podcast data \n",
    "After scraping podcast data from Chartable, we are going to start building the recommendation system enhanced with Langchain, and OpenAI \n",
    "We will be using the dataset scraped with title, description and genres"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Data Preprocessing** \n",
    "\n",
    "I first merge the text column into combined information including title of the podcast, overview and genres \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "podcast = podcast_scraper.load_podcast_json_to_dataframe('../data/podcast_data.json')\n",
    "podcast['combined'] = podcast.apply(lambda row: f\"Title: {row['podcast']}. Overview: {row['description']} Genres: {row['type']}\", axis=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Embedding** \n",
    "\n",
    "We're going to converting the texts into numerical representations referring to as embedding. Generally speaking, embedding helps perform tasks like retrieval which we will go through in the following code by converting unstructured texts into numerical representation. I'm using the text-embedding-3-small model which is a new model introduced by OpenAI early this year. For more information, you can check out [here](https://openai.com/index/new-embedding-models-and-api-updates/) \n",
    "\n",
    "We're going to store the vectors into a column called embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"OPENAI_API_KEY\"] = \"YOUR-OPENAI-API-KEY\"\n",
    "\n",
    "client = OpenAI()\n",
    "\n",
    "\n",
    "\n",
    "def get_embedding(text, model=\"text-embedding-3-small\"):\n",
    "   text = text.replace(\"\\n\", \" \")\n",
    "   return client.embeddings.create(input = [text], model=model).data[0].embedding\n",
    "\n",
    "podcast['embedding'] = podcast.combined.apply(lambda x: get_embedding(x, model='text-embedding-3-small'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Splitting**\n",
    "\n",
    "To fit into our model's context window, we would need to split the long podcast infromation into smaller meaningful chunks. There're different types of text splitters offered by Langchain. For more information, you can check out [here](https://python.langchain.com/v0.1/docs/modules/data_connection/document_transformers/). For this exercise, we will be using `RecursiveCharacterTextSplitter`. The advantage of this splitter is that it will try to keep all paragraphs together as long as possible to maintain strong context semantically. \n",
    "\n",
    "To access Langchain's text splitter, you would need to create a API key in their [website](https://www.langchain.com/)\n",
    "\n",
    "\n",
    "**Storing**\n",
    "Next we're going to store the splitted documents into a vector store. We will use Pinecone as vectorstore with OpenAI embeddings. First we will need to create an index in Pinecone [console](https://docs.pinecone.io/guides/indexes/create-an-index#create-a-serverless-index). \n",
    "Then we will use `from_documents` method to accepts the class objects created using Langchain's `RecursiveCharacterTextSplitter` class. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use text splitter to break down text on tokens and new lines \n",
    "\n",
    "embeddings = OpenAIEmbeddings()\n",
    "\n",
    "os.environ[\"PINECONE_API_KEY\"] = \"YOUR-PINECONE-API-KEY\"\n",
    "\n",
    "pc = Pinecone(api_key=os.environ[\"PINECONE_API_KEY\"])\n",
    "\n",
    "index_name = pc.Index('your-index-name')\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size = 1000,\n",
    "    chunk_overlap  = 100,\n",
    "    length_function = len,\n",
    "    add_start_index = True)\n",
    "\n",
    "documents = [Document(page_content=text) for text in podcast['combined'].tolist()]\n",
    "texts = text_splitter.split_documents(documents)\n",
    "\n",
    "\n",
    "vectorstore_from_docs = PineconeVectorStore.from_documents(\n",
    "    texts,\n",
    "    index_name='your-index-name',\n",
    "    embedding=embeddings\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Retrieval**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = vectorstore_from_docs.as_retriever()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "llm = ChatOpenAI(model_name='gpt-3.5-turbo', temperature=0)\n",
    "\n",
    "memory = ConversationBufferMemory(memory_key=\"chat_history\", return_messages= True)\n",
    "chain = ConversationalRetrievalChain.from_llm(llm, retriever= retriever, memory= memory)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I would recommend \"What Should I Read Next?\" podcast for finding book recommendations. Anne Bogel interviews readers about the books they love, hate, and are currently reading, then makes recommendations on what to read next. It\\'s specifically designed to help you find your next read.'"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = 'If I want to know what book i should read, which podcast would you recommend?'\n",
    "chain.run({'question': query})\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prompt engineering \n",
    "Pass custom prompt with information about users \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"\n",
    "You're a podcast recommender system that helps users to find the right podcast that match their interest. \n",
    "Use the following pieces of context to answer the question at the end.\n",
    "For each question, use the context and input provided by the user\n",
    "\n",
    "{context}\n",
    "\"\"\"\n",
    "user_info = \"\"\" The following are user input provided by the users \n",
    "\n",
    "Age: {age}\n",
    "Gender: {gender}\n",
    "Profession: {profession}\n",
    "Topics of interest: {interest}\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "template_suffix = \"\"\"\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
