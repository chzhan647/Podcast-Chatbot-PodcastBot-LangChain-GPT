{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import os \n",
    "from \n",
    "# from openai.embeddings_utils import get_embedding\n",
    "import tiktoken\n",
    "\n",
    "from langchain.vectorstores import Pinecone\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "import pinecone\n",
    "import podcast_scraper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "podcast = podcast_scraper.load_podcast_json_to_dataframe('../data/podcast_data.json')\n",
    "podcast['combined_info'] = podcast.apply(lambda row: f\"Title: {row['podcast']}. Overview: {row['description']} Genres: {row['type']}\", axis=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_encoding = \"cl100k_base\"  # this the encoding for text-embedding-ada-002\n",
    "max_tokens = 8000  # the maximum for text-embedding-ada-002 is 8191\n",
    "\n",
    "encoding = tiktoken.get_encoding(embedding_encoding)\n",
    "\n",
    "# omit descriptions that are too long to embed\n",
    "podcast[\"n_tokens\"] = podcast.combined_info.apply(lambda x: len(encoding.encode(x)))\n",
    "podcast = podcast[podcast.n_tokens <= max_tokens]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Storing and documenting in a vector store \n",
    "Use Pinecone as a vectorstore and OpenAI embeddings as embedding model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = OpenAIEmbeddings(openai_api_key=os.environ['API_KEY']) \n",
    "\n",
    "pinecone.init(\n",
    "api_key= os.environ['PINECONE_KEY'], environment= 'us-west1-gcp-free')\n",
    "index_name = pinecone.Index('index-podcast')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Retrieval and building retrieval chain\n",
    "Use Langchain retriever with Langchain ConversationalRetrieval chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectordb = Pinecone.from_documents(texts, embeddings, index_name='index-1')\n",
    "retriever = vectordb.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "\n",
    "memory = ConversationBufferMemory(memory_key=\"chat_history\", return_messages= True)\n",
    "chain = ConversationalRetrievalChain.from_llm(llm, retriever= retriever, memory= memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"I'm looking for a podcast similar to Conan O'Brian Needs a Friday. What would you suggest to me\"\n",
    "result = chain.run({'question': query})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prompt engineering \n",
    "Pass custom prompt with information about users \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"\n",
    "You're a podcast recommender system that helps users to find the right podcast that match their interest. \n",
    "Use the following pieces of context to answer the question at the end.\n",
    "For each question, use the context and input provided by the user\n",
    "\n",
    "{context}\n",
    "\"\"\"\n",
    "user_info = \"\"\" The following are user input provided by the users \n",
    "\n",
    "Age: {age}\n",
    "Gender: {gender}\n",
    "Profession: {profession}\n",
    "Topics of interest: {interest}\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "template_suffix = \"\"\"\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
